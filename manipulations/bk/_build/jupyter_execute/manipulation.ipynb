{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text extraction from ontology data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:  \n",
    "- Load the ontology\n",
    "- Understand its classes, properties and possible interconnections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../new_FROD_ontology.owl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''print('\\tClasses: ', *list(cls.name for cls in onto.classes()), sep='\\n\\t\\t')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    print('\\n\\tIndividuals: ', *list(ind.name for ind in onto.individuals()), sep='\\n\\t\\t')\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    print('\\n\\tProperties: ', *list(prop.name for prop in onto.properties()), sep='\\n\\t\\t')'''\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m onto\n\u001b[0;32m---> 10\u001b[0m onto \u001b[38;5;241m=\u001b[39m \u001b[43mlookAtTheOntology\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile://../new_FROD_ontology.owl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mlookAtTheOntology\u001b[0;34m(onto_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlookAtTheOntology\u001b[39m(onto_path):\n\u001b[0;32m----> 2\u001b[0m     onto \u001b[38;5;241m=\u001b[39m \u001b[43mget_ontology\u001b[49m\u001b[43m(\u001b[49m\u001b[43monto_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''print('\\tClasses: ', *list(cls.name for cls in onto.classes()), sep='\\n\\t\\t')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    print('\\n\\tIndividuals: ', *list(ind.name for ind in onto.individuals()), sep='\\n\\t\\t')\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    print('\\n\\tProperties: ', *list(prop.name for prop in onto.properties()), sep='\\n\\t\\t')'''\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m onto\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/owlready2/namespace.py:1090\u001b[0m, in \u001b[0;36mOntology.load\u001b[0;34m(self, only_local, fileobj, reload, reload_if_newer, url, **args)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reload \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mget_last_update_time() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m   1089\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _LOG_LEVEL: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m* Owlready2 *     ...loading ontology \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, f), file \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 1090\u001b[0m   fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1091\u001b[0m   \u001b[38;5;66;03m#try:     new_base_iri = self.graph.parse(fileobj, default_base = self._base_iri, **args)\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:     new_base_iri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mparse(fileobj, default_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_orig_base_iri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../new_FROD_ontology.owl'"
     ]
    }
   ],
   "source": [
    "def lookAtTheOntology(onto_path):\n",
    "    onto = get_ontology(onto_path).load()\n",
    "    \n",
    "    '''print('\\tClasses: ', *list(cls.name for cls in onto.classes()), sep='\\n\\t\\t')\n",
    "    print('\\n\\tIndividuals: ', *list(ind.name for ind in onto.individuals()), sep='\\n\\t\\t')\n",
    "    print('\\n\\tProperties: ', *list(prop.name for prop in onto.properties()), sep='\\n\\t\\t')'''\n",
    "\n",
    "    return onto\n",
    "\n",
    "onto = lookAtTheOntology('file://../new_FROD_ontology.owl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We were told to use a dataset similar to the ontology_  \n",
    "\n",
    "Performing text extraction using both an ontology and a dataset involves first integrating the dataset into the ontology by mapping the dataset's concepts to the ontology's classes and properties. \n",
    "\n",
    "Step 2: (Map dataset to Ontology classes and properties)  \n",
    "- Create corresponding classes and properties for the dataset\n",
    "- Create instances of the classes for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "banking_path = '../data/Fraud_banking.csv'\n",
    "csv_banking = pd.read_csv(banking_path)\n",
    "csv_banking = csv_banking.sample(10000).reset_index(drop=True)\n",
    "\n",
    "# Convert binary values to categorical values\n",
    "csv_banking[\"isFraud_Cat\"]        = csv_banking[\"isFraud\"].replace({1: \"Fraud\", 0: \"Not Fraud\"})\n",
    "csv_banking[\"isFlaggedFraud_Cat\"] = csv_banking[\"isFlaggedFraud\"].replace({1: \"Yes\", 0: \"No\"})\n",
    "csv_banking[\"type_num\"]           = csv_banking[\"type\"].replace({'CASH_OUT':1, 'CASH_IN':2, 'PAYMENT':3, 'DEBIT':4, 'TRANSFER':5})\n",
    "\n",
    "display(csv_banking.head(), csv_banking.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corresponding classes and properties for the chosen subclass of the ontology \n",
    "def addClassestoOntology(ontology, ontology_class, df):\n",
    "    # Get the list of columns in the dataframe\n",
    "    columns = list(df.columns)\n",
    "    # Check if the classes is in the ontology\n",
    "    if ontology_class in list(ontology.classes()):\n",
    "        for column in columns:\n",
    "            with ontology:\n",
    "                class column(DataProperty):\n",
    "                    domain  = [ontology_class]\n",
    "                    range = [df[column].dtype]\n",
    "    # Check if the classes and properties have been created\n",
    "    # --------------------------------------------------- Not sure how to do this yet \n",
    "    \n",
    "    return ontology;\n",
    "\n",
    "onto = addClassestoOntology(onto, 'Transaction', csv_banking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\t Instances created \n"
     ]
    }
   ],
   "source": [
    "# Create an entry key, value pair for each attribute in every row of the dataset\n",
    "def create_entry(df,index,columns=csv_banking.columns[1:]):\n",
    "    entry = {}\n",
    "    for x,y in zip(columns,df.iloc[index][columns]):\n",
    "        if type(y)==np.float64:\n",
    "            y = float(y)\n",
    "        elif type(y)==np.int64:\n",
    "            y=int(y)\n",
    "    \n",
    "        entry[x]=[y] \n",
    "    if 'zip' in entry:\n",
    "        entry['_zip']=entry['zip']\n",
    "        del entry['zip']\n",
    "    #print('Entry: ',entry, sep='\\t')\n",
    "    return entry\n",
    "\n",
    "# Create instances of the classes for each row of the dataset\n",
    "def createInstances(ontology, dataset, chosenClass):\n",
    "    with ontology:\n",
    "        for idx,_ in dataset.iterrows():\n",
    "            row_entry = create_entry(dataset, idx)\n",
    "            transaction = ontology.chosenClass(chosenClass+'_'+str(idx), **row_entry)\n",
    "            # print('Transaction: ', transaction, 'Entry', row_entry, sep='\\t')\n",
    "    # Check if the instances have been created\n",
    "    print(len(list(onto.Transaction.instances())), ' Instances created ', sep='\\t')\n",
    "    return ontology;\n",
    "\n",
    "onto_n = createInstances(onto, csv_banking, 'Transaction')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using extracted text to create rules for fraud detection\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step :  \n",
    "- define extraction rules based on the client's specifications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to focus on the Transaction class   \n",
    "\n",
    "_The client has chosen the classes to evaluate so from here on our data is irrelevant_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfScenarios = ['Identity_Theft','Credit_Card','Investment','Banking'] # The list of classes are the different fraud scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IdentityTheftRules(ontology): # Validate the identity Theft scenario with the rule statement ans send a notification to the user\n",
    "    # check what subclass of the ontology you are working with\n",
    "    if ontology.classes() == [ontology.Identity_Theft]:\n",
    "        # Create the rule statement\n",
    "        rule_statement1 = 'for x in ontology.Identity_Theft.instances():\\\n",
    "            if x.issued == \"France\" and x.ATM_Withdrawal_location_timezone == \"United States\":\\\n",
    "                print(\"Identity Theft\")'\n",
    "        rule_statement2 = 'for x in ontology.Identity_Theft.instances():\\\n",
    "            if x.issued == \"France\" and x.ATM_Withdrawal_location_timezone == \"Europe\":\\\n",
    "                pass'\n",
    "        rule_statement3 = 'for x in ontology.Identity_Theft.instances():\\\n",
    "            if x.subscription_payement in [\"RATP\",\"SNCF\",\"SFR\",\"FREEMOBILE\",\"LYCAMOBILE\"]:\\\n",
    "                print(\"Identity Theft\")'\n",
    "        rule_statement4 = 'for x in ontology.Identity_Theft.instances():\\\n",
    "            if x.insurance in [\"Orange\",\"Bouygues\",\"AXA\"]:\\\n",
    "                pass'\n",
    "        rule_statement5 = 'for x in ontology.Identity_Theft.instances():\\\n",
    "            if x.wrong_transaction_pin > 3:\\\n",
    "                print(\"Identity Theft , Blocked Account\")'\n",
    "        rule_statement6 = 'for x in ontology.Identity_Theft.instances():\\\n",
    "            if x.Account_holder_name == \"Adamu Hussaini\" and x.share_purchase == \"Adamu Hussaini\":\\\n",
    "                pass'\n",
    "        # Execute the rule statement\n",
    "        exec(rule_statement1)\n",
    "        exec(rule_statement2)\n",
    "        exec(rule_statement3)\n",
    "        exec(rule_statement4)\n",
    "        exec(rule_statement5)\n",
    "        exec(rule_statement6)\n",
    "    return \n",
    "\n",
    "def CreditCardRules(ontology): # Validate the credit card scenario with the rule statement ans send a notification to the user\n",
    "    # check what subclass of the ontology you are working with\n",
    "    if ontology.classes() == [ontology.Credit_Card]:\n",
    "        # Create the rule statements\n",
    "        rule_statement1 = 'for x in ontology.Credit_Card.instances():\\\n",
    "            if x.IP_address == \"18970988\" and x.geolocation == \"Paris\":\\\n",
    "                pass'\n",
    "        rule_statement2 = 'for x in ontology.Credit_Card.instances():\\\n",
    "            if x.IP_address == \"18970988\" and x.geolocation != \"Paris\":\\\n",
    "                print(\"Credit Card Fraud\")'\n",
    "        rule_statement3 = 'for x in ontology.Credit_Card.instances():\\\n",
    "            if x.Number_of_transactions > 3:\\\n",
    "                print(\"Credit Card Fraud\")'\n",
    "        rule_statement4 = 'for x in ontology.Credit_Card.instances():\\\n",
    "            if x.transaction_amount > 1000:\\\n",
    "                print(\"Credit Card Fraud\")'\n",
    "        rule_statement5 = 'for x in ontology.Credit_Card.instances():\\\n",
    "            if x.Account_holder_name == \"Adamu Hussaini\" and x.share_purchase != \"Adamu Hussaini\":\\\n",
    "                print(\"Identity Theft\")'\n",
    "        rule_statement6 = 'for x in ontology.Credit_Card.instances():\\\n",
    "            if x.online_payment_platform_address == \"France\" and x.online_payment_platform_address == \"Euro zone\":\\\n",
    "                pass'\n",
    "        rule_statement7 = 'for x in ontology.Credit_Card.instances():\\\n",
    "            if x.online_payment_platform_address in [\"Nigeria\",\"Asia\",\"North America\"]:\\\n",
    "                print(\"Credit Card Fraud\")'\n",
    "        rule_statement8 = 'for x in ontology.Credit_Card.instances():\\\n",
    "            if x.device_hash == \"Hidden\":\\\n",
    "                print(\"Credit Card Fraud\")'\n",
    "        # Execute the rule statement\n",
    "        exec(rule_statement1)\n",
    "        exec(rule_statement2)\n",
    "        exec(rule_statement3)\n",
    "        exec(rule_statement4)\n",
    "        exec(rule_statement5)\n",
    "        exec(rule_statement6)\n",
    "        exec(rule_statement7)\n",
    "        exec(rule_statement8)\n",
    "    return\n",
    "        \n",
    "def InvestmentRules(ontology): # Validate the investment scenario with the rule statement ans send a notification to the user\n",
    "    # check what subclass of the ontology you are working with\n",
    "    if ontology.classes() == [ontology.Investment]:\n",
    "        # Create the rule statements\n",
    "        rule_statement1 = 'for x in ontology.Investment.instances():\\\n",
    "            if x.Transaction == \"investment payment\" and x.payment_gateway == \"www.sharepurchase.com\":\\\n",
    "                print(\"Investment Fraud\")'\n",
    "        rule_statement2 = 'for x in ontology.Investment.instances():\\\n",
    "            if x.Transaction == \"investment scheme\":\\\n",
    "                print(\"Investment Fraud, Blocked\")'\n",
    "        rule_statement3 = 'for x in ontology.Investment.instances():\\\n",
    "            if x.Transaction_amount < 1000:\\\n",
    "                pass'\n",
    "        rule_statement4 = 'for x in ontology.Investment.instances():\\\n",
    "            if x.previous_transaction(\"Adamu Hussaini\") == \"charge back\":\\\n",
    "                print(\"Investment Fraud\")'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test the rules\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:  \n",
    "- Validate against the ontology.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdentityTheftRules(onto_n)\n",
    "CreditCardRules(onto_n)\n",
    "InvestmentRules(onto_n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Can't validate without appropriate data_   \n",
    "\n",
    "With the code, hopefully we have verified the extraction rules for the individuals.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Take away"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lessons learnt from working on the project:\n",
    "- Working with ontology data is crazy fun ... emphasis on __crazy__.  \n",
    "- When we break things down, the tasks become a whole less complicated and daunting.\n",
    "- It is important to write code in a modular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}